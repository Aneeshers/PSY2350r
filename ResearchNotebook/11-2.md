- 4 cues then drop 2, add 2 new (look at the paper)
- plot the value of the correct action of the Q table (plot the rewarded cues)
- This task is equivalent to the bandit task
- Read the paper to see how they penalize the mice
- How to map method. signature/ link them to hierarchical bayesian meta learn

![[Screenshot 2023-11-02 at 11.51.18 AM.png]]

We want our one hot encoding of the state --> we multiply that with some weight matrix that projects onto a two/three units, and then we have a Q matrix that projects that Z unit to the q table

-- update our Q table by doing gradient descent on the Q matrix as well as the W matrix (based of our loss on board)


### Loss Function:
We're using a mean squared error (MSE) loss function, which, for a single sample, is given by:

\[ $$L = (target - q\_values\_current[a])^2$$ \]

### Gradient with Respect to Q:
To update the matrix `Q`, we need to find how the loss changes as `Q` changes. This requires computing the partial derivative of the loss function with respect to each element of `Q`. Since only the Q-values for the action `a` are involved in computing the loss, we only need to update the column of `Q` corresponding to this action.

Abstractly, the gradient of the loss `L` with respect to the Q-values for action `a`, `q_values_current[a]`, is:

\[ $$\frac{\partial L}{\partial q\_values\_current[a]} = 2 \cdot (q\_values\_current[a] - target)$$ \]

However, we only use the term `(q_values_current[a] - target)` because the `2` is a constant scaling factor that can be absorbed into the learning rate `alpha`.

The actual Q-values are the product of the latent representation `z` and the `Q` matrix:

\[ $$q\_values\_current = z \cdot Q$$ \]

Therefore, to update `Q`, we consider how the loss changes as `z` changes, which gives us the gradient with respect to `Q`:

\[ $$grad\_Q = (target - q\_values\_current[a]) \cdot self.z$$ \]

Here, `self.z` is the latent representation for the current state, which was used to calculate `q_values_current[a]`. The gradient `grad_Q` is a vector that has the same dimension as `self.z` and tells us how to adjust the weights in `Q` that were used to compute `q_values_current[a]`.

### Gradient with Respect to W:
Updating the matrix `W` is slightly more involved because `W` affects the Q-values through the latent representation `z`. To find the gradient of `W`, we first consider how the loss `L` changes with respect to `z`, and then how `z` changes with respect to `W`.

Using the chain rule, the gradient of `L` with respect to `W` is:

\[ $$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial q\_values\_current[a]} \cdot \frac{\partial q\_values\_current[a]}{\partial z} \cdot \frac{\partial z}{\partial W}$$ \]

We already have the first term from the gradient with respect to `Q`. The second term, $\frac{\partial q\_values\_current[a]}{\partial z}$, is simply the column of `Q` corresponding to action `a`, because `q_values_current[a]` is computed as `z` dot `Q[:, a]`. The third term, $\frac{\partial z}{\partial W}$, involves the input state `one_hot_s` because `z` is computed as `one_hot_s` dot `W`.

Putting it all together, we have:

\[ $$grad\_W = (target - q\_values\_current[a]) \cdot one\_hot\_s \cdot self.Q[:, a]$$ \]

Here, the gradient `grad_W` will be a matrix with the same dimensions as `W` and describes how to adjust each weight in `W` to reduce the loss.

### Gradient Descent Updates:
Finally, we apply the gradient descent updates to `Q` and `W`:

\[ $$Q[:, a] = Q[:, a] - \alpha \cdot grad\_Q$$ \]
\[ $$W = W - \alpha \cdot grad\_W$$ \]

These updates adjust the weights in the direction that reduces the loss, scaled by the learning rate `alpha`.

In these updates, `-=` signifies that we're moving in the direction opposite to the gradient since we're performing gradient descent (minimizing the loss). The learning rate `alpha` determines the size of the update step. If `alpha` is too large, it might lead to overshooting the minimum of the loss function, which could be a factor in the exponential increase in Q-values you mentioned earlier.