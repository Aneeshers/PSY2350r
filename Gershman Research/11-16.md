Went over paper with Jay 
- need to revisit how to connect hierarchical Bayesian learning to our existing go/no go problem
- Confirm that Q learning bottle neck is doing that (set Q alpha to be half of or much lower than W alpha so that we learn our representation faster for a new cue)
- There is difference between probablilistic estimation and policy learning (because whether your Q value for the correct action is 1,0 vs. 0.6,0.4, you're still going to pick action 1, so there's not really any faster learning we can measure)
- Confirm with plots that bottle neck Q learning does in fact learn faster